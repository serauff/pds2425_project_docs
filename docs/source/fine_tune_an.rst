Fine-Tuning on small Dataset
============================

To begin our fine-tuning, we start with the generated dataset from the previous :doc:`data_generation` . Data is generated and expanded as
shown in the subsection :ref:`Dataset Annotation`. Every context has a single answer to a single question. The necessary structure for fine-tuning QA models is thus provided as

>>> [{question: 'question', context: 'context', answers: {'text':['text'], answer_start:[int]}}]

We use the question to prompt the model. Along with this prompt, the context from which the question is to be extracted is passed as well. This leads to our expected answers stored in the answers column.

With the `ast` and `Datasets` library, we can load our dataframe into a dataset object by calling

>>> import pandas as pd
>>> import ast
>>> dataset = pd.read_csv('df_expand.csv')
>>> dataset['answers'] = dataset['answers'].apply(lambda x: ast.literal_eval(x)) #Make sure our answers column is a dict and not str.
>>> ds = Dataset.from_pandas(pd.read_csv('df_expand.csv'))

The following section is adapted from the `NLP Course for Question Answering Chapter 7/7 from Huggingface <https://huggingface.co/learn/nlp-course/chapter7/7>`_

As our initial dataset is rather small and thus we value training points over evaluation possibilities, we aim for a test split of only 10%.

The train/test split is generated by

>>> from sklearn.model_selection import train_test_split
>>> ds.train_test_split(test_size=0.1)

.. _pre and post:

Pre & Postprocessing Data
-------------------------

We aim to fine-tune two models on the initial Dataset. For the first model, we load the base bert model

>>> from transformers import AutoTokenizer
>>> model_checkpoint = "bert-base-cased"
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

For the second model, we modify the model_checkpoint to

>>> model_checkpoint = "timpal0l/mdeberta-v3-base-squad2"

`mdeberta v3 base squad2 <https://huggingface.co/timpal0l/mdeberta-v3-base-squad2>`_ is a multilingual version of DeBERTa. We choose a multilingual version, as our training data has some specialties such as german names.

Then, we continue with preprocessing our dataset according to the `NLP Course <https://huggingface.co/learn/nlp-course/chapter7/7>`_

.. autofunction:: ft_an.preprocess_training_examples

.. autofunction:: ft_an.preprocess_validation_examples

After that, these functions get applied on the training and validation datasets. As demonstrated in the `NLP Course <https://huggingface.co/learn/nlp-course/chapter7/7#processing-the-training-data>`_, we apply the function on the train and test sets with the `.map()` function.

For example with the validation set:

>>> val_dataset = datasets["validation"].map(prepocess_validation_examples,
                                             batched=True,
                                             remove_columns=datasets["validation"].column_names)

.. _Model Training:

Model Training
--------------

For training our two models, it is necessary to log into huggingface, which can be achieved by generating a HFTOKEN and 

>>> from huggingface_hub import notebook_login
>>> notebook_login()

Then, we proceed to train our first model with

>>> from transformers import TrainingArguments
>>> args = TrainingArguments(
         "s3auf/bert-finetuned-busiQA",
          evaluation_strategy="no",
          save_strategy="epoch",
          learning_rate=2e-5,
          num_train_epochs=3,
          weight_decay=0.01,
          fp16=True,
          push_to_hub=True,
)

Then, the Trainer module from the transformers library can be used to pass the model, training arguments, the relevant datasets as well as the tokenizer to the Trainer API.

for the second model, we rerun our preprocessing with the tokenizers and model_checkpoint of the `timpal0l/mdeberta-v3-base-squad2 <https://huggingface.co/timpal0l/mdeberta-v3-base-squad2>`_ model and change the args model name parameter to `"s3auf/mdeberta-v3-squad2-ft-busiQA-3ep"`.

Regenerate Dataset
------------------

To finally generate a large datset which we can later use for fine-tuning, we use similar methods as in the :ref:`Prompting Gemini via API` . However, instead of first generating full-text questions with `gemini-1.5-flash` we utilise the model to create new json questionnaires in the format of the ones provided by snapADDY. For this, the :py:func:`data_gen.init_context` is utilised. 

For example, the context can be initialised with

>>> rules = "Type of questionnaire fields can be either 'SINGLE_SELECT', 'MULTI_SELECT', 'TEXT' or 'NUMBER'"
>>> questionnaire_format = "..." #Multiple rows of the questionnaire in the desired output format.
>>> context = f"You are a helpful assistant in generating questionnaires in a business context. Generate similar questionnaires in the following JSON format: {questionnaire_format}/n {rules}/n {data}"
>>> chat = reset_context() #clean any previous context
>>> chat = init_context(context)

We most importantly also pass some example data to the model, consisting out of the initial questionnaires by snapADDY. The context is further extended with the provided questionnaires in JSON format. 

After JSON outputs are generated by passing a chat message and the initialised chat to the :py:func:`generate_with_msg` multiple times, the structure of the outputs are cleaned with regEx as long as deemed useful and afterwards get manually refined.

Then we use the same logic as described in :doc:`data_generation` to create new Question-Answer pairs with our synthetic questionnaires. This results in 2266 Question-Answer pairs that are ready to be annotated by our fine-tuned models in the :ref:`annotator` .

.. _annotator:

The Annotator
-------------

Following the concept of this project, we aim to automatically annotate a generated Dataset for later fine-tuning purposes. This is, because manual annotation of answers requires intensive work. We try to accomplish good results with using multiple models to extract answers from a QA-Set and then annotating the QA-Set with an answer as well as the answer start index.

For this, a function is created to programmatically call a model and append the answer and confidence scores of the model for the extraction. After this, a postprocessing function is used to rank the extracted answers according to the models confidence estimation. Thus, the highest confidence score is selected as the annotated answer.

.. autofunction:: ft_an.annotator

The :py:func:`ft_an.annotator` is used to accept a DataFrame containing questions and contexts. It also expects a model_checkpoint for calling ``pipeline("question-answering, model = model_checkpoint)``. It then appends the score of the model in the column ``"score_" + model_checkpoint`` and the answer dict in the column ``"answers_" + model_checkpoint``.

For using this function, we pass our dataset that needs to be annotated multiple times.

Models used to annotate the dataset:

-`s3auf/mdeberta-v3-squad2-ft-busiQA-3ep <https://huggingface.co/s3auf/mdeberta-v3-squad2-ft-busiQA-3ep>`_ (base fine-tuned on squadv2, then on our initial dataset)

-`timpal0l/mdeberta-v3-base-squad2 <https://huggingface.co/timpal0l/mdeberta-v3-base-squad2>`_ (base fine-tuned on squadv2)

-`s3auf/bert-finetuned-busiQA <https://huggingface.co/s3auf/bert-finetuned-busiQA>`_ (base bert fine-tuned on our initial dataset)

.. _ranker:

The Ranker
----------

In order to get the best possible answers we leverage the :py:func:`ft_an.rank_answers` function to select the best annotations of the dataset.
The function scans a provided dataframe for score and answer columns by checking if they include the strings (eg. "score_" from the previous :py:func:`ft_an.annotator` function.
Following, it iterates through the score and answer columns and selects the best element from each row. This is then appended to a list of best answers. It also preserves the confidence score each model had.

Example use:

>>> df_an = annotator(pd.read_csv(dataset), "s3auf/mdeberta-v3-squad2-ft-busiQA-3ep")
>>> df_an = annotator(df_an, "s3auf/bert-finetuned-busiQA")
>>> df_ranked = rank_answers(df_an)

.. autofunction:: ft_an.rank_answers

.. _dataset gen:

Generating the Dataset
----------------------

To provide the datset, we can now use the ``datasets`` and ``sklearn.model_selection`` library for generating a dataset that is ready to be used on `huggingface.co <huggingface.co>`_.

The dataset is loaded, split into a ``train_test_split(test_size = 0.3, seed = 42, shuffle = True)``. With this, we ensure that every time we train a model, we train on the same splits.

>>> from datasets import DatasetDict, load_dataset
>>> from sklearn.model_selection import train_test_split
>>> ds = load_dataset("csv", data_files="df_synth_r.csv")
>>> ds_train = ds['train].train_test_split(test_size=0.3, seed=42, shuffle=True)
>>> ds_splits = DatasetDict({
    'train': ds_train['train'],
    'valid': ds_train['test']
})

This leads to our ``DatasetDict``

DatasetDict({
    train: Dataset({
        features: ['question', 'context', 'option', 'type', 'answer_length', 'answers_s3auf_mdeberta-v3-squad2-ft-busiQA-3ep', 'score_s3auf_mdeberta-v3-squad2-ft-busiQA-3ep', 'answers_s3auf/bert-finetuned-busiQA', 'score_s3auf/bert-finetuned-busiQA', 'answers_timpal0l/mdeberta-v3-base-squad2', 'score_timpal0l/mdeberta-v3-base-squad2', 'ranked_answer', 'ranked_score'],
        num_rows: 1586
    })
    valid: Dataset({
        features: ['question', 'context', 'option', 'type', 'answer_length', 'answers_s3auf_mdeberta-v3-squad2-ft-busiQA-3ep', 'score_s3auf_mdeberta-v3-squad2-ft-busiQA-3ep', 'answers_s3auf/bert-finetuned-busiQA', 'score_s3auf/bert-finetuned-busiQA', 'answers_timpal0l/mdeberta-v3-base-squad2', 'score_timpal0l/mdeberta-v3-base-squad2', 'ranked_answer', 'ranked_score'],
        num_rows: 680
    })
})

For offline use, we then save this dataset to the disc with 

>>> ds_splits.save_to_disk('business-questionnaire-dataset')

For online use, we push the dataset to the hub. It will be publicly available from 2nd of February 2025.

>>> from huggingface_hub import create_repo, upload_folder, notebook_login
>>> ds_splits.push_to_hub("s3auf/business-questionnaire-pds")

.. autosummary::
   :toctree: generated
