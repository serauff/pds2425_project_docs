Getting Started
===============
To begin our fine-tuning, we start with the generated dataset from the previous :doc:other_cat section. Data is generated and expanded as
shown in the subsection :ref:`Dataset Annotation`. Every context has a single answer to a single question. The necessary structure for fine-tuning QA models is thus provided as

>>> [{question: 'question', context: 'context', answers: {'text':['text'], answer_start:[int]}}]

We use the question to prompt the model. Along with this prompt, the context from which the question is to be extracted is passed as well. This leads to our expected answers stored in the answers column.

With the `ast` and `Datasets` library, we can load our dataframe into a dataset object by calling

>>> import pandas as pd
>>> import ast
>>> dataset = pd.read_csv('df_expand.csv')
>>> dataset['answers'] = dataset['answers'].apply(lambda x: ast.literal_eval(x)) #Make sure our answers column is a dict and not str.
>>> ds = Dataset.from_pandas(pd.read_csv('df_expand.csv'))

The following section is adapted from the `NLP Course for Question Answering Chapter 7/7 from Huggingface <https://huggingface.co/learn/nlp-course/chapter7/7>`_

As our initial dataset is rather small and thus we value training points over evaluation possibilities, we aim for a test split of only 10%.

The train/test split is generated by

>>> from sklearn.model_selection import train_test_split
>>> ds.train_test_split(test_size=0.1)

.. _pre and post:

Pre & Postprocessing Data
-------------------------

We aim to fine-tune two models on the initial Dataset. For the first model, we load the base bert model

>>> from transformers import AutoTokenizer
>>> model_checkpoint = "bert-base-cased"
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

For the second model, we modify the model_checkpoint to

>>> model_checkpoint = "timpal0l/mdeberta-v3-base-squad2"

`mdeberta v3 base squad2 <https://huggingface.co/timpal0l/mdeberta-v3-base-squad2>`_ is a multilingual version of DeBERTa. We choose a multilingual version, as our training data has some specialties such as german names.

Then, we continue with preprocessing our dataset according to the `NLP Course <https://huggingface.co/learn/nlp-course/chapter7/7>`_

.. autofunction:: ft_an.preprocess_training_examples

.. autofunction:: ft_an.preprocess_validation_examples

After that, these functions get applied on the training and validation datasets. As demonstrated in the `NLP Course <https://huggingface.co/learn/nlp-course/chapter7/7#processing-the-training-data>`_, we apply the function on the train and test sets with the `.map()` function.

For example with the validation set:

>>> val_dataset = datasets["validation"].map(prepocess_validation_examples,
                                             batched=True,
                                             remove_columns=datasets["validation"].column_names)

.. _Model Training:

Model Training:
---------------

For training our two models, it is necessary to log into huggingface, which can be achieved by generating a HFTOKEN and 

>>> from huggingface_hub import notebook_login
>>> notebook_login()

Then, we proceed to train our first model with

>>> from transformers import TrainingArguments
>>> args = TrainingArguments(
         "s3auf/bert-finetuned-busiQA",
          evaluation_strategy="no",
          save_strategy="epoch",
          learning_rate=2e-5,
          num_train_epochs=3,
          weight_decay=0.01,
          fp16=True,
          push_to_hub=True,
)

Then, the Trainer module from the transformers library can be used to pass the model, training arguments, the relevant datasets as well as the tokenizer to the Trainer API.

for the second model, we rerun our preprocessing with the tokenizers and model_checkpoint of the `timpal0l/mdeberta-v3-base-squad2 <https://huggingface.co/timpal0l/mdeberta-v3-base-squad2>`_ model and change the args model name parameter to `"s3auf/mdeberta-v3-squad2-ft-busiQA-3ep"`.

.. _annotator:

The Annotator
-------------

Following the concept of this project, we aim to automatically annotate a generated Dataset for later fine-tuning purposes. This is, because manual annotation of answers requires intensive work. We try to accomplish good results with using multiple models to extract answers from a QA-Set and then annotating the QA-Set with an answer as well as the answer start index.

For this, a function is created to programmatically call a model and append the answer and confidence scores of the model for the extraction. After this, a postprocessing function is used to rank the extracted answers according to the models confidence estimation. Thus, the highest confidence score is selected as the annotated answer.

.. autofunction:: ft_an.annotator

The py:func:`ft_an.annotator` is used to accept a DataFrame containing questions and contexts. It also expects a model_checkpoint for calling ``pipeline("question-answering, model = model_checkpoint)``. It then appends the score of the model in the column ``"score_" + model_checkpoint`` and the answer dict in the column ``"answers_" + model_checkpoint``.

For using this function, we pass our dataset that needs to be annotated multiple times.

Models used to annotate the dataset:
-`s3auf/mdeberta-v3-squad2-ft-busiQA-3ep <https://huggingface.co/s3auf/mdeberta-v3-squad2-ft-busiQA-3ep>`_ (base fine-tuned on squadv2, then on our initial dataset)
-`timpal0l/mdeberta-v3-base-squad2 <https://huggingface.co/timpal0l/mdeberta-v3-base-squad2>`_ (base fine-tuned on squadv2)
-`s3auf/bert-finetuned-busiQA <https://huggingface.co/s3auf/bert-finetuned-busiQA>`_ (base bert fine-tuned on our initial dataset)

.. _ranker:

The Ranker
----------

In order to get the best possible answers we leverage the py:func:`ft_an.rank_answers` function to select the best annotations of the dataset.
The function scans a provided dataframe for score and answer columns by checking if they include the strings (eg. "score_" from the previous py:func:`ft_an.annotator` function.
Following, it iterates through the score and answer columns and selects the best element from each row. This is then appended to a list of best answers. It also preserves the confidence score each model had.

Example use:

>>> df_an = annotator(pd.read_csv(dataset), "s3auf/mdeberta-v3-squad2-ft-busiQA-3ep")
>>> df_an = annotator(df_an, "s3auf/bert-finetuned-busiQA")
>>> df_ranked = rank_answers(df_an)

.. autofunction:: ft_an.rank_answers


.. autosummary::
   :toctree: generated
