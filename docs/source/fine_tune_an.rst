Getting Started
===============
To begin our fine-tuning, we start with the generated dataset from the previous :doc:other_cat section. Data is generated and expanded as
shown in the subsection :ref:`Dataset Annotation`. Every context has a single answer to a single question. The necessary structure for fine-tuning QA models is thus provided as

>>> [{question: 'question', context: 'context', answers: {'text':['text'], answer_start:[int]}}]

We use the question to prompt the model. Along with this prompt, the context from which the question is to be extracted is passed as well. This leads to our expected answers stored in the answers column.

With the `ast` and `Datasets` library, we can load our dataframe into a dataset object by calling

>>> import pandas as pd
>>> import ast
>>> dataset = pd.read_csv('df_expand.csv')
>>> dataset['answers'] = dataset['answers'].apply(lambda x: ast.literal_eval(x)) #Make sure our answers column is a dict and not str.
>>> ds = Dataset.from_pandas(pd.read_csv('df_expand.csv'))

The following section is adapted from the `NLP Course for Question Answering Chapter 7/7 from Huggingface <https://huggingface.co/learn/nlp-course/chapter7/7>`_

As our initial dataset is rather small and thus we value training points over evaluation possibilities, we aim for a test split of only 10%.

The train/test split is generated by

>>> from sklearn.model_selection import train_test_split
>>> ds.train_test_split(test_size=0.1)

.. _pre and post:

Pre & Postprocessing Data
-------------------------

We aim to fine-tune two models on the initial Dataset. For the first model, we load the base bert model

>>> from transformers import AutoTokenizer
>>> model_checkpoint = "bert-base-cased"
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

For the second model, we modify the model_checkpoint to

>>> model_checkpoint = "timpal0l/mdeberta-v3-base-squad2"

`mdeberta v3 base squad2 <https://huggingface.co/timpal0l/mdeberta-v3-base-squad2>`_ is a multilingual version of DeBERTa. We choose a multilingual version, as our training data has some specialties such as german names.

Then, we continue with preprocessing our dataset according to the `NLP Course <https://huggingface.co/learn/nlp-course/chapter7/7>`_

.. autofunction:: ft_an.preprocess_training_examples

.. autofunction:: ft_an.preprocess_validation_examples

After that, these functions get applied on the training and validation datasets. As demonstrated in the `NLP Course <https://huggingface.co/learn/nlp-course/chapter7/7#processing-the-training-data>`_, we apply the function on the train and test sets with the `.map()` function.

For example with the validation set:

>>> val_dataset = datasets["validation"].map(prepocess_validation_examples,
                                             batched=True,
                                             remove_columns=datasets["validation"].column_names)

.. _Model Training:

Model Training:
---------------

For training our two models, it is necessary to log into huggingface, which can be achieved by generating a HFTOKEN and 

>>> from huggingface_hub import notebook_login
>>> notebook_login()

Then, we proceed to train our first model with

>>> from transformers import TrainingArguments
>>> args = TrainingArguments(
         "s3auf/bert-finetuned-busiQA",
          evaluation_strategy="no",
          save_strategy="epoch",
          learning_rate=2e-5,
          num_train_epochs=3,
          weight_decay=0.01,
          fp16=True,
          push_to_hub=True,
)

Then, the Trainer module from the transformers library can be used to pass the model, training arguments, the relevant datasets as well as the tokenizer to the Trainer API.

for the second model, we rerun our preprocessing with the tokenizers and model_checkpoint of the `timpal0l/mdeberta-v3-base-squad2 <https://huggingface.co/timpal0l/mdeberta-v3-base-squad2>`_ model and change the args model name parameter to `"s3auf/mdeberta-v3-squad2-ft-busiQA-3ep"`.

.. _annotator:

The Annotator
-------------

Following the concept of this project, we aim to automatically annotate a generated Dataset for later fine-tuning purposes. This is, because manual annotation of answers requires intensive work. We try to accomplish good results with using multiple models to extract answers from a QA-Set and then annotating the QA-Set with an answer as well as the answer start index.



Usage
=====

.. _installation:

Installation
------------

To use Lumache, first install it using pip:

.. code-block:: console

   (.venv) $ pip install lumache

Creating recipes
----------------

To retrieve a list of random ingredients,
you can use the ``lumache.get_random_ingredients()`` function:

.. autofunction:: data_test.get_random_ingredients

The ``kind`` parameter should be either ``"meat"``, ``"fish"``,
or ``"veggies"``. Otherwise, :py:func:`lumache.get_random_ingredients`
will raise an exception.

.. autoexception:: data_test.InvalidKindError

For example:

>>> import lumache
>>> lumache.get_random_ingredients()
['shells', 'gorgonzola', 'parsley']

Ranking answers
---------------
To retrieve a list of random ingredients,
you can use the ``data_gen.rank_answers(df)`` function:

.. autofunction:: data_gen.rank_answers

.. autosummary::
   :toctree: generated

   lumache
