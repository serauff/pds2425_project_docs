Getting Started
===============
To begin our fine-tuning, we start with the generated dataset from the previous :doc:other_cat section. Data is generated and expanded as
shown in the subsection :ref:`Dataset Annotation`. Every context has a single answer to a single question. The necessary structure for fine-tuning QA models is thus provided as

>>> [{question: 'question', context: 'context', answers: {'text':['text'], answer_start:[int]}}]

We use the question to prompt the model. Along with this prompt, the context from which the question is to be extracted is passed as well. This leads to our expected answers stored in the answers column.

With the `ast` and `Datasets` library, we can load our dataframe into a dataset object by calling

>>> import pandas as pd
>>> import ast
>>> dataset = pd.read_csv('df_expand.csv')
>>> dataset['answers'] = dataset['answers'].apply(lambda x: ast.literal_eval(x)) #Make sure our answers column is a dict and not str.
>>> ds = Dataset.from_pandas(pd.read_csv('df_expand.csv'))

The following section is adapted from the `NLP Course for Question Answering Chapter 7/7 from Huggingface <https://huggingface.co/learn/nlp-course/chapter7/7>`_

As our initial dataset is rather small and thus we value training points over evaluation possibilities, we aim for a test split of only 10%.

The train/test split is generated by

>>> from sklearn.model_selection import train_test_split
>>> ds.train_test_split(test_size=0.1)

.. _Model Training:

Model Training
--------------

We aim to fine-tune two models on the initial Dataset. For the first model, we load the base bert model

>>> from transformers import AutoTokenizer
>>> model_checkpoint = "bert-base-cased"
>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

For the second model, we modify the model_checkpoint to

>>> model_checkpoint = "timpal0l/mdeberta-v3-base-squad2"

`mdeberta v3 base squad2 <https://huggingface.co/timpal0l/mdeberta-v3-base-squad2>`_ is a multilingual version of DeBERTa. We choose a multilingual version, as our training data has some specialties such as german names.

Then, we continue wit preprocessing our dataset according to the `NLP Course <https://huggingface.co/learn/nlp-course/chapter7/7>`_

Usage
=====

.. _installation:

Installation
------------

To use Lumache, first install it using pip:

.. code-block:: console

   (.venv) $ pip install lumache

Creating recipes
----------------

To retrieve a list of random ingredients,
you can use the ``lumache.get_random_ingredients()`` function:

.. autofunction:: data_test.get_random_ingredients

The ``kind`` parameter should be either ``"meat"``, ``"fish"``,
or ``"veggies"``. Otherwise, :py:func:`lumache.get_random_ingredients`
will raise an exception.

.. autoexception:: data_test.InvalidKindError

For example:

>>> import lumache
>>> lumache.get_random_ingredients()
['shells', 'gorgonzola', 'parsley']

Ranking answers
---------------
To retrieve a list of random ingredients,
you can use the ``data_gen.rank_answers(df)`` function:

.. autofunction:: data_gen.rank_answers

.. autosummary::
   :toctree: generated

   lumache
